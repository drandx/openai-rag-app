{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=BblV6AQsd2s&ab_channel=RiseAgainstVEVO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the model\n",
    "Define the LLM model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There is currently no scientific evidence to definitively prove or disprove that we are living in a simulation. The idea of a simulated reality is a philosophical concept that has been explored in various theories, but it remains a topic of debate and speculation. Ultimately, whether or not we are in a simulation is a question that may never be fully answered.', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "model.invoke(\"Are we in a simulation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the magic of Chains :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is currently no scientific evidence to definitively prove or disprove the idea that we are living in a simulation. Some researchers and philosophers have proposed the concept of a simulated reality, but it remains a speculative and philosophical question. Ultimately, it is up to individual belief and interpretation.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Chain example\n",
    "chain = model | parser\n",
    "\n",
    "chain.invoke(\"Are we in a simulation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "  Answer the question based on the context below. If you can't\n",
      "  answer the question, just say \"I don't know\".\n",
      "\n",
      "  Context: The sky is blue.\n",
      "\n",
      "  Question: What color is the sky?\n",
      "  \n",
      "Did it work?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No, you have an appointment at 3pm tomorrow so you cannot go for lunch at that time.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "  Answer the question based on the context below. If you can't\n",
    "  answer the question, just say \"I don't know\".\n",
    "\n",
    "  Context: {context}\n",
    "\n",
    "  Question: {question}\n",
    "  \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"The sky is blue.\", question=\"What color is the sky?\"))\n",
    "\n",
    "print(\"Did it work?\")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "chain.invoke({\n",
    "  \"context\": \"I have an appointment tomorrow at 3pm.\", \n",
    "  \"question\": \"Can I go for lunch tomorrow at 3:00PM?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Respuesta: No, tienes una cita a las 3pm ma√±ana.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "translation_template = \"\"\"\n",
    "  Translate the \n",
    "\n",
    "  answer: {answer}\n",
    "\n",
    "  to\n",
    "\n",
    "  language: {language}\n",
    "  \"\"\"\n",
    "\n",
    "translation_prompt = ChatPromptTemplate.from_template(translation_template)\n",
    "\n",
    "translation_chain = (\n",
    "  {\"answer\": chain, \"language\": itemgetter(\"language\") } | translation_prompt | model | parser\n",
    ")\n",
    "\n",
    "translation_chain.invoke({\n",
    "  \"context\": \"I have an appointment tomorrow at 3pm.\", \n",
    "  \"question\": \"Can I go for lunch tomorrow at 3:00PM?\",\n",
    "  \"language\": \"es\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribing the YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import whisper\n",
    "from pytube import YouTube\n",
    "\n",
    "if not os.path.exists(\"transcription.txt\"):\n",
    "  youtube = YouTube(YOUTUBE_VIDEO)\n",
    "  audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "  whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "  script_dir = os.path.expanduser('~/Documents/tmp')\n",
    "\n",
    "  with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    print(f\"** Downloading {audio.title} to {temp_dir}\")\n",
    "    # Download the audio file with this name transcription_audio.mp4\n",
    "    file = audio.download(temp_dir, filename=\"transcription_audio.mp4\")\n",
    "    \n",
    "    # Print the file path\n",
    "    print(file)\n",
    "    # Validate if the file exists\n",
    "    print(os.path.exists(file))\n",
    "    # Print the file name\n",
    "    print(os.path.basename(file))\n",
    "\n",
    "    transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
    "    \n",
    "    with open(\"transcription.txt\", \"w\") as f:\n",
    "     f.write(transcription)\n",
    "\n",
    "    transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"This video is only under Stunde Oh Am I loud and clear I'm not breaking up, am I still your charm? I'm not just that luck, are we getting closer? Are we just getting more lost? I'll show you mine if you show me yours first Let's compare scars, I'll tell you who's It is worse, let's unlight these pages and replace them with the wrong ones We live on front porches and swim like a boy We get by just fine here, I've been in the way The flow is a labor, I'll sleep till the end I'll cross these streets until you hold my hand I've been here so long, I think that it's time to move The winter's so cold, summer's over too soon Let's pack our bags and settle down where palm trees grow I've got some friends, some that I hardly know But we've had some times, I wouldn't trade for the world We chase these days down with talks of the places that we will go We live on front porches and swim like a boy We get by just fine here, I've been in the way The flow is a labor, I'll sleep till the end I'll cross these streets until you hold my hand Until you hold my hand I'll show you mine, if you show me yours first Let's compare scars, I'll tell you, it's worse and lots Sunrise these pages and replace them with a round one We live on front porches and swim like a boy We get by just fine here, I'll never know where it's The flow is a labor, I'll sleep till the end I won't cross these streets until you hold my hand We live on front porches and swim like a boy\", metadata={'source': 'transcription.txt'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"This video is only under Stunde Oh Am I loud and clear I'm not breaking up, am I still your charm?\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"I still your charm? I'm not just that luck, are we getting closer? Are we just getting more lost?\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"getting more lost? I'll show you mine if you show me yours first Let's compare scars, I'll tell you\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"I'll tell you who's It is worse, let's unlight these pages and replace them with the wrong ones We\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"the wrong ones We live on front porches and swim like a boy We get by just fine here, I've been in\", metadata={'source': 'transcription.txt'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "text_splitter.split_documents(text_documents)[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
