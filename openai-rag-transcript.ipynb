{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=BblV6AQsd2s&ab_channel=RiseAgainstVEVO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the model\n",
    "Define the LLM model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The idea that we might be living in a simulation, similar to the concept portrayed in movies like The Matrix, is a topic of debate among scientists and philosophers. Some argue that advancements in technology and the possibility of creating highly realistic virtual worlds suggest that it is theoretically possible for us to be living in a simulated reality. However, there is currently no definitive evidence to prove or disprove this theory. Ultimately, the question of whether we are in a simulation remains a speculative and philosophical one.', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "model.invoke(\"Are we in a simulation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the magic of Chains :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a philosophical question that has been debated by many thinkers. Some believe that it is possible that we are living in a simulation created by a more advanced civilization or technology, while others argue that there is no evidence to support this theory. Ultimately, it is a question that may never have a definitive answer.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Chain example\n",
    "chain = model | parser\n",
    "\n",
    "chain.invoke(\"Are we in a simulation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "  Answer the question based on the context below. If you can't\n",
      "  answer the question, just say \"I don't know\".\n",
      "\n",
      "  Context: The sky is blue.\n",
      "\n",
      "  Question: What color is the sky?\n",
      "  \n",
      "Did it work?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No, you have an appointment at 3pm tomorrow.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "  Answer the question based on the context below. If you can't\n",
    "  answer the question, just say \"I don't know\".\n",
    "\n",
    "  Context: {context}\n",
    "\n",
    "  Question: {question}\n",
    "  \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"The sky is blue.\", question=\"What color is the sky?\"))\n",
    "\n",
    "print(\"Did it work?\")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "chain.invoke({\n",
    "  \"context\": \"I have an appointment tomorrow at 3pm.\", \n",
    "  \"question\": \"Can I go for lunch tomorrow at 3:00PM?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Respuesta: No, tienes una cita a las 3pm ma√±ana, por lo que no puedes ir a almorzar a esa hora.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "translation_template = \"\"\"\n",
    "  Translate the \n",
    "\n",
    "  answer: {answer}\n",
    "\n",
    "  to\n",
    "\n",
    "  language: {language}\n",
    "  \"\"\"\n",
    "\n",
    "translation_prompt = ChatPromptTemplate.from_template(translation_template)\n",
    "\n",
    "translation_chain = (\n",
    "  {\"answer\": chain, \"language\": itemgetter(\"language\") } | translation_prompt | model | parser\n",
    ")\n",
    "\n",
    "translation_chain.invoke({\n",
    "  \"context\": \"I have an appointment tomorrow at 3pm.\", \n",
    "  \"question\": \"Can I go for lunch tomorrow at 3:00PM?\",\n",
    "  \"language\": \"es\"\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
